{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2e5b60a0a08e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "# choose a GPU card\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PolicyNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.inputs = tf.placeholder(tf.float32, [None,state_size], name = \"inputs_\")\n",
    "                # Jieda note: we are using sparse_softmax_cross_entropy_with_logits, so action is now a scaler (instead of a vector)\n",
    "                self.actions = tf.placeholder(tf.int32, [None,], name =\"actions\")\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards_\")\n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc1 = tf.layers.dense(inputs=self.inputs,\n",
    "                                          units = 256, activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc1\")\n",
    "\n",
    "            with tf.name_scope(\"fc2\"):\n",
    "                self.fc2 = tf.layers.dense(inputs = self.fc1,\n",
    "                                           units = 256, activation = tf.nn.relu,\n",
    "                                           kernel_initializer = tf.contrib.layers.xavier_initializer(), name = 'fc2')\n",
    "\n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc2,\n",
    "                                              units = action_size,\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                              activation = None)\n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, labels =self.actions)\n",
    "                self.weighted_negative_likelihoods = tf.multiply(self.cross_entropy, self.discounted_episode_rewards)\n",
    "                self.loss = tf.reduce_mean(self.weighted_negative_likelihoods)\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# environment parameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "possible_actions = np.identity(action_size,dtype=int).tolist()\n",
    "\n",
    "# training hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 500\n",
    "batch_size = 1000 # each 1 is a timestep (not an episode)\n",
    "\n",
    "\n",
    "## Helper function\n",
    "### calculates the discounted total rewards from current step onward\n",
    "\n",
    "def discount_rewards(r, gamma=0.95, normalization=False):\n",
    "    \"\"\"\n",
    "    computes the discounted rewards from time t onward for a given episode\n",
    "    length of r corresponds to the number of steps in an episode\n",
    "    discounted_r has the same length --> discounted rewards at each time step\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    if normalization:\n",
    "        mean = np.mean(discounted_r)\n",
    "        std = np.std(discounted_r)\n",
    "        discounted_r = (discounted_r - mean) / (std)\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================\n",
    "# run policy\n",
    "#============================\n",
    "def make_batch(batch_size):\n",
    "\n",
    "    \"\"\"\n",
    "    We will run the policy and generate a bunch of episodes\n",
    "    We need to keep track of (st,at,rt,st+1) for each of the episode, those we will use for training\n",
    "    :param batch_size: number of episodes in a batch\n",
    "    :return: 3 list: states, actions, rewards of batch (each value is accumulated discounted total rewards)\n",
    "    \"\"\"\n",
    "    # initialize lists:\n",
    "\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    # number of episode in batch\n",
    "    episode_num = 1\n",
    "\n",
    "    # Get a new state\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # run state through policy and calculate action\n",
    "        action_probability_distribution = sess.run(PolicyNetwork.action_distribution, feed_dict={PolicyNetwork.inputs: state.reshape(1,state_size)})\n",
    "\n",
    "        # choose action\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())\n",
    "\n",
    "        # perform action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # the 3 lists here tracks the quantity for each episode\n",
    "        # the other 2 lists (rewards_of_batch and discounted_rewards, they are both list of list)\n",
    "        # they contain num_episode list and each list is the rewards of that episode of each time stamp\n",
    "        # rewards_of_batch is UNdiscounted, discounted_rewards is DISCOUNTED and counts from time t onwards\n",
    "        # in the calculation of loss function, we need the discounted_rewards\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # if the entire episode is done\n",
    "\n",
    "            # if you have more than 1 episode in a batch, we track rewards of each episode in the batch\n",
    "            # this is undiscounted rewards, we WILL use this to do the plotting\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "\n",
    "            # disc_rwds_per_episode is a list with each element being the discounted reward from a time stamp t onward\n",
    "            disc_rwds_per_episode = discount_rewards(rewards_of_episode, gamma=0.99, normalization=True)\n",
    "            # discounted_rewards is a list of lenth (number of episode)\n",
    "            discounted_rewards.append(disc_rwds_per_episode)\n",
    "            #\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "            # reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            # add episode\n",
    "            episode_num += 1\n",
    "            # reset the state\n",
    "            state = env.reset()\n",
    "\n",
    "        else:\n",
    "            # if not done, the next_state become the current state\n",
    "            state = next_state\n",
    "\n",
    "    # Essentially, we will keep records of (state, action, rewards), need those for training!\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards),episode_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the graph\n",
    "# tf.reset_default_graph()\n",
    "#==============================\n",
    "#  Initialize network and session\n",
    "#==============================\n",
    "# Instantiate the PolicyNetwork\n",
    "PolicyNetwork = PolicyNetwork(state_size, action_size, learning_rate)\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================\n",
    "# Training and Printing some stats\n",
    "#===================\n",
    "allRewards =[]\n",
    "total_rewards =0\n",
    "maximumRewardRecorded =0\n",
    "mean_reward_total =[]\n",
    "epoch =10\n",
    "average_reward=[]\n",
    "training = True\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    while epoch < num_epochs +1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(100)\n",
    "        # total rewards of the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # calculate the mean reward of the batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # calculate the average reward of all training\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # maximum reward recorded\n",
    "        max_reward_recorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"===============================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # feedforward, gradient and backprop\n",
    "        loss_,_ = sess.run([PolicyNetwork.loss,PolicyNetwork.train_opt], \\\n",
    "                feed_dict={PolicyNetwork.inputs: states_mb, PolicyNetwork.actions: actions_mb, PolicyNetwork.discounted_episode_rewards: discounted_rewards_mb})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
