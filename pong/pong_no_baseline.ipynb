{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "# choose a GPU card\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the is the pre-processing function for pre-processing the image (which is our state) from 210x160x3 into 6400 (80*80) 2D float array\n",
    "def preprocess(image):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "  image = image[35:195] # crop\n",
    "  image = image[::2,::2,0] # downsample by factor of 2\n",
    "  image[image == 144] = 0 # erase background (background type 1)\n",
    "  image[image == 109] = 0 # erase background (background type 2)\n",
    "  image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    \n",
    "  return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load environment\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "# environment parameters\n",
    "state_size = [80,80]\n",
    "# Jieda: here we only need to choose between action [RIGHT, LEFT]\n",
    "# original action space has 6 actions\n",
    "# action_size = env.action_space.n\n",
    "action_size =2\n",
    "possible_actions = np.identity(action_size,dtype=int).tolist()\n",
    "\n",
    "# training hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 500\n",
    "batch_size = 1000 # each 1 is a timestep (not an episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PolicyNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                # Jieda note: state is image, size [210, 160, 3]\n",
    "                self.inputs = tf.placeholder(tf.float32, [None,*state_size], name = \"inputs\")\n",
    "                # Jieda note: we are using sparse_softmax_cross_entropy_with_logits, so action is now a scaler (instead of a vector)\n",
    "                self.actions = tf.placeholder(tf.int32, [None,], name =\"actions\")\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards_\")\n",
    "                # Jieda note: place holder for the ValueNetwork esitimated value\n",
    "                # Jieda note: following line is commented out, because feed the results from dis_sample_total_rewards - value_estimates\n",
    "                # directly to discounted_episode_rewards\n",
    "                # self.value_estimate = tf.placeholder(tf.float32, [None,], name=\"value_estimate_\")\n",
    "            \n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs, filters=32, kernel_size=[8,8],strides=[4,4],padding=\"VALID\",\\\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(), name = \"conv1\")\n",
    "                \n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name = 'batch_norm1')\n",
    "                \n",
    "                self.conv1_out = tf.nn.relu(self.conv1_batchnorm, name='conv1_out')\n",
    "             \n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                \n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out, filters=64,\n",
    "                                             kernel_size=[4,4], strides=[2,2], padding=\"VALID\",\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv2\")\n",
    "                \n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name='batch_norm2')\n",
    "                \n",
    "                self.conv2_out = tf.nn.relu(self.conv2_batchnorm, name='conv2_out')\n",
    "            \n",
    "            with tf.name_scope(\"conv3\"):\n",
    "            \n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out, filters=64,\n",
    "                                             kernel_size=[3,3], strides=[1,1], padding=\"VALID\",\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv3\")\n",
    "                \n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name='batch_norm3')\n",
    "                \n",
    "                self.conv3_out = tf.nn.relu(self.conv3_batchnorm, name='conv3_out')\n",
    "            \n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "                \n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc1 = tf.layers.dense(inputs=self.flatten,\n",
    "                                          units = 512, activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc1\")\n",
    "\n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc1,\n",
    "                                              units = action_size,\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                              activation = None)\n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, labels =self.actions)\n",
    "                # Jieda noted: for baseline, we subtract the value_estimate_ from discounted_episode_rewards\n",
    "                self.weighted_negative_likelihoods = tf.multiply(self.cross_entropy, self.discounted_episode_rewards)\n",
    "                self.loss = tf.reduce_mean(self.weighted_negative_likelihoods)\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = preprocess(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Helper function\n",
    "### calculates the discounted total rewards from current step onward\n",
    "\n",
    "def discount_rewards(r, gamma=0.95, normalization=False):\n",
    "    \"\"\"\n",
    "    computes the discounted rewards from time t onward for a given episode\n",
    "    length of r corresponds to the number of steps in an episode\n",
    "    discounted_r has the same length --> discounted rewards at each time step\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    if normalization:\n",
    "        mean = np.mean(discounted_r)\n",
    "        std = np.std(discounted_r)\n",
    "        discounted_r = (discounted_r - mean) / (std)\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================\n",
    "# run policy\n",
    "#============================\n",
    "def make_batch(batch_size):\n",
    "\n",
    "    \"\"\"\n",
    "    We will run the policy and generate a bunch of episodes\n",
    "    We need to keep track of (st,at,rt,st+1) for each of the episode, those we will use for training\n",
    "    :param batch_size: number of episodes in a batch\n",
    "    :return: 3 list: states, actions, rewards of batch (each value is accumulated discounted total rewards)\n",
    "    \"\"\"\n",
    "    # initialize lists:\n",
    "\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    # number of episode in batch\n",
    "    episode_num = 1\n",
    "\n",
    "    # Get a new state\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # run state through policy and calculate action\n",
    "        action_probability_distribution = sess.run(PolicyNetwork.action_distribution, feed_dict={PolicyNetwork.inputs: state.reshape(1,state_size)})\n",
    "\n",
    "        # choose action\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())\n",
    "\n",
    "        # perform action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # the 3 lists here tracks the quantity for each episode\n",
    "        # the other 2 lists (rewards_of_batch and discounted_rewards, they are both list of list)\n",
    "        # they contain num_episode list and each list is the rewards of that episode of each time stamp\n",
    "        # rewards_of_batch is UNdiscounted, discounted_rewards is DISCOUNTED and counts from time t onwards\n",
    "        # in the calculation of loss function, we need the discounted_rewards\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # if the entire episode is done\n",
    "\n",
    "            # if you have more than 1 episode in a batch, we track rewards of each episode in the batch\n",
    "            # this is undiscounted rewards, we WILL use this to do the plotting\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "\n",
    "            # disc_rwds_per_episode is a list with each element being the discounted reward from a time stamp t onward\n",
    "            disc_rwds_per_episode = discount_rewards(rewards_of_episode, gamma=0.99, normalization=True)\n",
    "            # discounted_rewards is a list of lenth (number of episode)\n",
    "            discounted_rewards.append(disc_rwds_per_episode)\n",
    "            #\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "            # reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            # add episode\n",
    "            episode_num += 1\n",
    "            # reset the state\n",
    "            state = env.reset()\n",
    "\n",
    "        else:\n",
    "            # if not done, the next_state become the current state\n",
    "            state = next_state\n",
    "\n",
    "    # Essentially, we will keep records of (state, action, rewards), need those for training!\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards),episode_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'list'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1215\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    717\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5b9b522a7462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#==============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Instantiate the PolicyNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mPolicyNetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b9a3daec1dd2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size, learning_rate, name)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# Jieda note: state is image, size [210, 160, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inputs_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;31m# Jieda note: we are using sparse_softmax_cross_entropy_with_logits, so action is now a scaler (instead of a vector)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"actions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   2617\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   2618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2619\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   6665\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6666\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6667\u001b[0;31m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6668\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   6669\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[0;31mTypeError\u001b[0m: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'list'."
     ]
    }
   ],
   "source": [
    "# reset the graph\n",
    "# tf.reset_default_graph()\n",
    "#==============================\n",
    "#  Initialize network and session\n",
    "#==============================\n",
    "# Instantiate the PolicyNetwork\n",
    "PolicyNetwork = PolicyNetwork(state_size, action_size, learning_rate)\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================\n",
    "# Training and Printing some stats\n",
    "#===================\n",
    "allRewards =[]\n",
    "total_rewards =0\n",
    "maximumRewardRecorded =0\n",
    "mean_reward_total =[]\n",
    "num_epochs =500\n",
    "average_reward=[]\n",
    "training = True\n",
    "epoch = 1\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    while epoch < num_epochs +1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(1000)\n",
    "        # total rewards of the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # calculate the mean reward of the batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # calculate the average reward of all training\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # maximum reward recorded\n",
    "        max_reward_recorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"===============================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # feedforward, gradient and backprop\n",
    "        loss_,_ = sess.run([PolicyNetwork.loss,PolicyNetwork.train_opt], \\\n",
    "                feed_dict={PolicyNetwork.inputs: states_mb, PolicyNetwork.actions: actions_mb, PolicyNetwork.discounted_episode_rewards: discounted_rewards_mb})\n",
    "\n",
    "        # update epoch\n",
    "        epoch +=1\n",
    "\n",
    "# plot the average episode reward vs epoch\n",
    "# episode reward is the total undiscounted reward for an episode\n",
    "plt.plot(range(0,len(mean_reward_total)), mean_reward_total)\n",
    "plt.savefig('./pong_average_reward_no_baseline.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
