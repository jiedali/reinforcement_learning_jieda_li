{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "# choose a GPU card\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the is the pre-processing function for pre-processing the image (which is our state) from 210x160x3 into 6400 (80*80) 2D float array\n",
    "def preprocess(image):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "  image = image[35:195] # crop\n",
    "  image = image[::2,::2,0] # downsample by factor of 2\n",
    "  image[image == 144] = 0 # erase background (background type 1)\n",
    "  image[image == 109] = 0 # erase background (background type 2)\n",
    "  image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    \n",
    "  return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load environment\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "# environment parameters\n",
    "state_size = [80,80]\n",
    "# Jieda: here we only need to choose between action [RIGHT, LEFT]\n",
    "# original action space has 6 actions\n",
    "# action_size = env.action_space.n\n",
    "action_size =2\n",
    "# action_size =6\n",
    "possible_actions = np.identity(action_size,dtype=int).tolist()\n",
    "\n",
    "# training hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 5000\n",
    "batch_size = 1000 # each 1 is a timestep (not an episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PolicyNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                # Jieda note: state is image, size [210, 160, 3]\n",
    "                self.inputs = tf.placeholder(tf.float32, [None,*state_size], name = \"inputs\")\n",
    "                # Jieda note: we are using sparse_softmax_cross_entropy_with_logits, so action is now a scaler (instead of a vector)\n",
    "                self.actions = tf.placeholder(tf.int32, [None,], name =\"actions\")\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards_\")\n",
    "                # Jieda note: place holder for the ValueNetwork esitimated value\n",
    "                # Jieda note: following line is commented out, because feed the results from dis_sample_total_rewards - value_estimates\n",
    "                # directly to discounted_episode_rewards\n",
    "                # self.value_estimate = tf.placeholder(tf.float32, [None,], name=\"value_estimate_\")\n",
    "            \n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                self.conv1 = tf.layers.conv1d(inputs = self.inputs, filters=32, kernel_size=8,strides=4,padding=\"VALID\",\\\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(), name = \"conv1\")\n",
    "                \n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name = 'batch_norm1')\n",
    "                \n",
    "                self.conv1_out = tf.nn.relu(self.conv1_batchnorm, name='conv1_out')\n",
    "             \n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                \n",
    "                self.conv2 = tf.layers.conv1d(inputs = self.conv1_out, filters=64,\n",
    "                                             kernel_size=4, strides=2, padding=\"VALID\",\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv2\")\n",
    "                \n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name='batch_norm2')\n",
    "                \n",
    "                self.conv2_out = tf.nn.relu(self.conv2_batchnorm, name='conv2_out')\n",
    "            \n",
    "            with tf.name_scope(\"conv3\"):\n",
    "            \n",
    "                self.conv3 = tf.layers.conv1d(inputs = self.conv2_out, filters=64,\n",
    "                                             kernel_size=3, strides=1, padding=\"VALID\",\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv3\")\n",
    "                \n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name='batch_norm3')\n",
    "                \n",
    "                self.conv3_out = tf.nn.relu(self.conv3_batchnorm, name='conv3_out')\n",
    "            \n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "                \n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc1 = tf.layers.dense(inputs=self.flatten,\n",
    "                                          units = 512, activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc1\")\n",
    "\n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc1,\n",
    "                                              units = action_size,\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                              activation = None)\n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, labels =self.actions)\n",
    "                # Jieda noted: for baseline, we subtract the value_estimate_ from discounted_episode_rewards\n",
    "                self.weighted_negative_likelihoods = tf.multiply(self.cross_entropy, self.discounted_episode_rewards)\n",
    "                self.loss = tf.reduce_mean(self.weighted_negative_likelihoods)\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function\n",
    "### calculates the discounted total rewards from current step onward\n",
    "\n",
    "def discount_rewards(r, gamma=0.95, normalization=False):\n",
    "    \"\"\"\n",
    "    computes the discounted rewards from time t onward for a given episode\n",
    "    length of r corresponds to the number of steps in an episode\n",
    "    discounted_r has the same length --> discounted rewards at each time step\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    if normalization:\n",
    "        mean = np.mean(discounted_r)\n",
    "        std = np.std(discounted_r)\n",
    "        discounted_r = (discounted_r - mean) / (std)\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================\n",
    "# run policy\n",
    "#============================\n",
    "def make_batch(batch_size):\n",
    "\n",
    "    \"\"\"\n",
    "    We will run the policy and generate a bunch of episodes\n",
    "    We need to keep track of (st,at,rt,st+1) for each of the episode, those we will use for training\n",
    "    :param batch_size: number of episodes in a batch\n",
    "    :return: 3 list: states, actions, rewards of batch (each value is accumulated discounted total rewards)\n",
    "    \"\"\"\n",
    "    # initialize lists:\n",
    "\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    # number of episode in batch\n",
    "    episode_num = 1\n",
    "\n",
    "    # Get a new state\n",
    "    state = env.reset()\n",
    "    state = preprocess(state)\n",
    "\n",
    "    while True:\n",
    "        # run state through policy and calculate action\n",
    "        action_probability_distribution = sess.run(PolicyNetwork.action_distribution, feed_dict={PolicyNetwork.inputs: state.reshape(1,80,80)})\n",
    "#         print(\"debug, action probability distribution\")\n",
    "#         print(action_probability_distribution)\n",
    "\n",
    "#         print(\"debug:action_probability_distribution.ravel()[2:4]\")\n",
    "#         print(action_probability_distribution.ravel()[2:4])\n",
    "        # normalize the probability to sum to 1\n",
    "#         p_normalized = action_probability_distribution.ravel()[2:4]/sum(action_probability_distribution.ravel()[2:4])\n",
    "#         print(\"debug: normalized probability\")\n",
    "#         print(p_normalized)\n",
    "        # Note: if we don't normalize, probability for action 2 and 3 don't sum to 1 (because there are 6 possibility)\n",
    "        # choose action\n",
    "        # only choose between action 2 and 3 ('RIGHT' or 'LEFT')\n",
    "        action = np.random.choice([0,1], p=action_probability_distribution.ravel())\n",
    "        # map the action 0 to 2, 1 to 3 (2 in env definition is RIGHT, 3 is LEFT)\n",
    "        # this mapping is ONLY needed to perform the action in environment\n",
    "        # when we feed the action back to the PolicyNetwork, we will keep the action as 0 or 1\n",
    "        if action ==0:\n",
    "            action_env_compatible =2\n",
    "        elif action ==1:\n",
    "            action_env_compatible=3\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # perform action\n",
    "        next_state, reward, done, info = env.step(action_env_compatible)\n",
    "        # process next_state to shape (80,80)\n",
    "        next_state = preprocess(next_state)\n",
    "\n",
    "        # the 3 lists here tracks the quantity for each episode\n",
    "        # the other 2 lists (rewards_of_batch and discounted_rewards, they are both list of list)\n",
    "        # they contain num_episode list and each list is the rewards of that episode of each time stamp\n",
    "        # rewards_of_batch is UNdiscounted, discounted_rewards is DISCOUNTED and counts from time t onwards\n",
    "        # in the calculation of loss function, we need the discounted_rewards\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # if the entire episode is done\n",
    "\n",
    "            # if you have more than 1 episode in a batch, we track rewards of each episode in the batch\n",
    "            # this is undiscounted rewards, we WILL use this to do the plotting\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "\n",
    "            # disc_rwds_per_episode is a list with each element being the discounted reward from a time stamp t onward\n",
    "            disc_rwds_per_episode = discount_rewards(rewards_of_episode, gamma=0.99, normalization=True)\n",
    "            # discounted_rewards is a list of lenth (number of episode)\n",
    "            discounted_rewards.append(disc_rwds_per_episode)\n",
    "            #\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "            # reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            # add episode\n",
    "            episode_num += 1\n",
    "            # reset the state\n",
    "            state = env.reset()\n",
    "            state = preprocess(state)\n",
    "\n",
    "        else:\n",
    "            # if not done, the next_state become the current state\n",
    "            state = next_state\n",
    "\n",
    "    # Essentially, we will keep records of (state, action, rewards), need those for training!\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards),episode_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-9aa5dc227d41>:22: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv1D` instead.\n",
      "WARNING:tensorflow:From /nfs/home/jlg7773/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:218: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-9aa5dc227d41>:27: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From /nfs/home/jlg7773/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-9aa5dc227d41>:66: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /nfs/home/jlg7773/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# reset the graph\n",
    "tf.reset_default_graph()\n",
    "#==============================\n",
    "#  Initialize network and session\n",
    "#==============================\n",
    "# Instantiate the PolicyNetwork\n",
    "PolicyNetwork = PolicyNetwork(state_size, action_size, learning_rate)\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Epoch:  1 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  2 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  3 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  4 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  5 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  6 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  7 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  8 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  9 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  10 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  11 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  12 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  13 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  14 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  15 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  16 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  17 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  18 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  19 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  20 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  21 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  22 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  23 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  24 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  25 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -21.0\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  26 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.96153846153846\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  27 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.962962962962962\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  28 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.964285714285715\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  29 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.96551724137931\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  30 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.966666666666665\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  31 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.93548387096774\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  32 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.9375\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  33 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.939393939393938\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  34 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.91176470588235\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  35 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.885714285714286\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  36 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.88888888888889\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  37 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.864864864864863\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  38 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.81578947368421\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  39 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.76923076923077\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  40 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.775\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  41 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.78048780487805\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  42 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.761904761904763\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  43 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.767441860465116\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  44 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.772727272727273\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  45 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.77777777777778\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  46 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.782608695652176\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  47 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.76595744680851\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  48 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.75\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  49 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.73469387755102\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  50 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -19.0\n",
      "Mean Reward of that batch -19.0\n",
      "Average Reward of all training: -20.7\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  51 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.686274509803923\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  52 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.692307692307693\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  53 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.641509433962263\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  54 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.62962962962963\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  55 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.618181818181817\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  56 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.625\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  57 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.614035087719298\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  58 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.603448275862068\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  59 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -18.0\n",
      "Mean Reward of that batch -18.0\n",
      "Average Reward of all training: -20.559322033898304\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  60 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.566666666666666\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  61 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.557377049180328\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  62 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.548387096774192\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  63 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.53968253968254\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  64 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.53125\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  65 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.53846153846154\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  66 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.545454545454547\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  67 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.55223880597015\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  68 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.558823529411764\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  69 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.565217391304348\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  70 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.557142857142857\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  71 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.56338028169014\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  72 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.569444444444443\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  73 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.575342465753426\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  74 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.56756756756757\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  75 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.573333333333334\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  76 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.57894736842105\n",
      "Max reward for a batch so far: 0\n",
      "===============================\n",
      "Epoch:  77 / 5000\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.584415584415584\n",
      "Max reward for a batch so far: 0\n"
     ]
    }
   ],
   "source": [
    "#===================\n",
    "# Training and Printing some stats\n",
    "#===================\n",
    "allRewards =[]\n",
    "total_rewards =0\n",
    "maximumRewardRecorded =0\n",
    "mean_reward_total =[]\n",
    "num_epochs =5000\n",
    "average_reward=[]\n",
    "training = True\n",
    "epoch = 1\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    while epoch < num_epochs +1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(1000)\n",
    "        # total rewards of the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # calculate the mean reward of the batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # calculate the average reward of all training\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # maximum reward recorded\n",
    "        max_reward_recorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"===============================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # feedforward, gradient and backprop\n",
    "        loss_,_ = sess.run([PolicyNetwork.loss,PolicyNetwork.train_opt], \\\n",
    "                feed_dict={PolicyNetwork.inputs: states_mb, PolicyNetwork.actions: actions_mb, PolicyNetwork.discounted_episode_rewards: discounted_rewards_mb})\n",
    "\n",
    "        # update epoch\n",
    "        epoch +=1\n",
    "\n",
    "# plot the average episode reward vs epoch\n",
    "# episode reward is the total undiscounted reward for an episode\n",
    "plt.plot(range(0,len(mean_reward_total)), mean_reward_total)\n",
    "plt.savefig('./pong_average_reward_no_baseline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, ..., 2, 3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1105,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_of_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0., -1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_of_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
