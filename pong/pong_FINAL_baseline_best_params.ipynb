{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "# choose a GPU card\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is Pong with a ValueNetwork baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the is the pre-processing function for pre-processing the image (which is our state) from 210x160x3 into 6400 (80*80) 2D float array\n",
    "def preprocess(image):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "  image = image[35:195] # crop\n",
    "  image = image[::2,::2,0] # downsample by factor of 2\n",
    "  image[image == 144] = 0 # erase background (background type 1)\n",
    "  image[image == 109] = 0 # erase background (background type 2)\n",
    "  image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    \n",
    "  return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load environment\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "# environment parameters\n",
    "state_size = [80,80,1]\n",
    "# Jieda: here we only need to choose between action [RIGHT, LEFT]\n",
    "# original action space has 6 actions\n",
    "# action_size = env.action_space.n\n",
    "action_size =2\n",
    "# action_size =6\n",
    "possible_actions = np.identity(action_size,dtype=int).tolist()\n",
    "\n",
    "# training hyperparameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 6000\n",
    "batch_size = 1000 # each 1 is a timestep (not an episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PolicyNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                # Jieda note: state is image, size [210, 160, 3]\n",
    "                self.inputs = tf.placeholder(tf.float32, [None,*state_size], name = \"inputs\")\n",
    "                # Jieda note: we are using sparse_softmax_cross_entropy_with_logits, so action is now a scaler (instead of a vector)\n",
    "                self.actions = tf.placeholder(tf.int32, [None,], name =\"actions\")\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards_\")\n",
    "                # Jieda note: place holder for the ValueNetwork esitimated value\n",
    "                # Jieda note: following line is commented out, because feed the results from dis_sample_total_rewards - value_estimates\n",
    "                # directly to discounted_episode_rewards\n",
    "                # self.value_estimate = tf.placeholder(tf.float32, [None,], name=\"value_estimate_\")\n",
    "            \n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs, filters=32, kernel_size=[8,8],strides=[4,4],padding=\"VALID\",\\\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(), name = \"conv1\")\n",
    "                \n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name = 'batch_norm1')\n",
    "                \n",
    "                self.conv1_out = tf.nn.relu(self.conv1_batchnorm, name='conv1_out')\n",
    "             \n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                \n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out, filters=64,\n",
    "                                             kernel_size=[4,4], strides=[2,2],padding=\"VALID\",\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv2\")\n",
    "                \n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name='batch_norm2')\n",
    "                \n",
    "                self.conv2_out = tf.nn.relu(self.conv2_batchnorm, name='conv2_out')\n",
    "            \n",
    "            with tf.name_scope(\"conv3\"):\n",
    "            \n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out, filters=64,\n",
    "                                             kernel_size=[3,3], strides=[1,1], padding=\"VALID\",\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv3\")\n",
    "                \n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name='batch_norm3')\n",
    "                \n",
    "                self.conv3_out = tf.nn.relu(self.conv3_batchnorm, name='conv3_out')\n",
    "            \n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "                \n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc1 = tf.layers.dense(inputs=self.flatten,\n",
    "                                          units = 512, activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc1\")\n",
    "\n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc1,\n",
    "                                              units = action_size,\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                              activation = None)\n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, labels =self.actions)\n",
    "                # Jieda noted: for baseline, we subtract the value_estimate_ from discounted_episode_rewards\n",
    "                self.weighted_negative_likelihoods = tf.multiply(self.cross_entropy, self.discounted_episode_rewards)\n",
    "                self.loss = tf.reduce_mean(self.weighted_negative_likelihoods)\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(object):\n",
    "    \n",
    "    def __init__(self, state_size, learning_rate, name='ValueNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                # Jieda note: state is image, size [210, 160, 3]\n",
    "                self.inputs = tf.placeholder(tf.float32, [None,*state_size], name = \"inputs\")\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards_\")\n",
    "                self.target = tf.placeholder(tf.float32, [None,], name = \"target\")\n",
    "\n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs, filters=32, kernel_size=[8,8],strides=[4,4],padding=\"VALID\",\\\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(), name = \"conv1\")\n",
    "\n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name = 'batch_norm1')\n",
    "\n",
    "                self.conv1_out = tf.nn.relu(self.conv1_batchnorm, name='conv1_out')\n",
    "\n",
    "            with tf.name_scope(\"conv2\"):\n",
    "\n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out, filters=64,\n",
    "                                             kernel_size=[4,4], strides=[2,2], padding=\"VALID\",\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv2\")\n",
    "\n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name='batch_norm2')\n",
    "\n",
    "                self.conv2_out = tf.nn.relu(self.conv2_batchnorm, name='conv2_out')\n",
    "\n",
    "            with tf.name_scope(\"conv3\"):\n",
    "\n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out, filters=64,\n",
    "                                             kernel_size=[3,3], strides=[1,1], padding=\"VALID\",\n",
    "                                             kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv3\")\n",
    "\n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                                    training=True,\n",
    "                                                                    epsilon=1e-5,\n",
    "                                                                    name='batch_norm3')\n",
    "\n",
    "                self.conv3_out = tf.nn.relu(self.conv3_batchnorm, name='conv3_out')\n",
    "\n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "\n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc1 = tf.layers.dense(inputs=self.flatten,\n",
    "                                          units = 512, activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc1\")\n",
    "\n",
    "\n",
    "            with tf.name_scope(\"output\"):\n",
    "                self.output_layer = tf.layers.dense(\n",
    "                inputs=self.fc1,\n",
    "                units = 1,\n",
    "                activation = None,\n",
    "                kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"output\")\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.value_estimate = tf.squeeze(self.output_layer)\n",
    "                self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "    # method to update the Value neuron network parameters\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = {self.inputs: state, self.target:target}\n",
    "        _, loss = sess.run([self.train_opt, self.loss], feed_dict)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function\n",
    "### calculates the discounted total rewards from current step onward\n",
    "\n",
    "def discount_rewards(r, gamma=0.99, normalization=False):\n",
    "    \"\"\"\n",
    "    computes the discounted rewards from time t onward for a given episode\n",
    "    length of r corresponds to the number of steps in an episode\n",
    "    discounted_r has the same length --> discounted rewards at each time step\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    if normalization:\n",
    "        mean = np.mean(discounted_r)\n",
    "        std = np.std(discounted_r)\n",
    "        discounted_r = (discounted_r - mean) / (std)\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================\n",
    "# run policy\n",
    "#============================\n",
    "def make_batch(seed,batch_size):\n",
    "\n",
    "    \"\"\"\n",
    "    We will run the policy and generate a bunch of episodes\n",
    "    We need to keep track of (st,at,rt,st+1) for each of the episode, those we will use for training\n",
    "    :param batch_size: number of episodes in a batch\n",
    "    :return: 3 list: states, actions, rewards of batch (each value is accumulated discounted total rewards)\n",
    "    \"\"\"\n",
    "    # initialize lists:\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    # number of episode in batch\n",
    "    episode_num = 1\n",
    "\n",
    "    # Get a new state\n",
    "    env.seed(seed)\n",
    "    state = env.reset()\n",
    "    state = preprocess(state)\n",
    "    state = state.reshape(80,80,1)\n",
    "\n",
    "    while True:\n",
    "        # run state through policy and calculate action\n",
    "        action_probability_distribution = sess.run(PolicyNetwork.action_distribution, feed_dict={PolicyNetwork.inputs: state.reshape(1,80,80,1)})\n",
    "        action = np.random.choice([0,1], p=action_probability_distribution.ravel())\n",
    "        \n",
    "        # choose action\n",
    "        # only choose between action 2 and 3 ('RIGHT' or 'LEFT')\n",
    "        \n",
    "        # map the action 0 to 2, 1 to 3 (2 in env definition is RIGHT, 3 is LEFT)\n",
    "        # this mapping is ONLY needed to perform the action in environment\n",
    "        # when we feed the action back to the PolicyNetwork, we will keep the action as 0 or 1\n",
    "        if action ==0:\n",
    "            action_env_compatible =2\n",
    "        elif action ==1:\n",
    "            action_env_compatible=3\n",
    "            \n",
    "        # perform action\n",
    "        next_state, reward, done, info = env.step(action_env_compatible)\n",
    "        # process next_state to shape (80,80)\n",
    "        next_state = preprocess(next_state)\n",
    "        next_state = next_state.reshape(80,80,1)\n",
    "\n",
    "        # the 3 lists here tracks the quantity for each episode\n",
    "        # the other 2 lists (rewards_of_batch and discounted_rewards, they are both list of list)\n",
    "        # they contain num_episode list and each list is the rewards of that episode of each time stamp\n",
    "        # rewards_of_batch is UNdiscounted, discounted_rewards is DISCOUNTED and counts from time t onwards\n",
    "        # in the calculation of loss function, we need the discounted_rewards\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # if the entire episode is done\n",
    "\n",
    "            # if you have more than 1 episode in a batch, we track rewards of each episode in the batch\n",
    "            # this is undiscounted rewards, we WILL use this to do the plotting\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "\n",
    "            # disc_rwds_per_episode is a list with each element being the discounted reward from a time stamp t onward\n",
    "            disc_rwds_per_episode = discount_rewards(rewards_of_episode, gamma=0.99, normalization=True)\n",
    "            # discounted_rewards is a list of lenth (number of episode)\n",
    "            discounted_rewards.append(disc_rwds_per_episode)\n",
    "            #\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "            # reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            # add episode\n",
    "            episode_num += 1\n",
    "            # reset the state\n",
    "            state = env.reset()\n",
    "            state = preprocess(state)\n",
    "            state = state.reshape(80,80,1)\n",
    "\n",
    "        else:\n",
    "            # if not done, the next_state become the current state\n",
    "            state = next_state\n",
    "\n",
    "    # Essentially, we will keep records of (state, action, rewards), need those for training!\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards),episode_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-8e4160fffbb4>:22: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /nfs/home/jlg7773/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-8e4160fffbb4>:27: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From /nfs/home/jlg7773/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-8e4160fffbb4>:66: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /nfs/home/jlg7773/.conda/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# reset the graph\n",
    "tf.reset_default_graph()\n",
    "#==============================\n",
    "#  Initialize network and session\n",
    "#==============================\n",
    "# Instantiate the PolicyNetwork\n",
    "PolicyNetwork = PolicyNetwork(state_size, action_size, learning_rate)\n",
    "ValueNetwork = ValueNetwork(state_size, learning_rate)\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Epoch:  1 / 10\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.0\n",
      "Max reward for a batch so far: -20.0\n",
      "===============================\n",
      "Epoch:  2 / 10\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.5\n",
      "Max reward for a batch so far: -20.0\n",
      "===============================\n",
      "Epoch:  3 / 10\n",
      "Number of training episodes: 1\n",
      "Total reward: -20.0\n",
      "Mean Reward of that batch -20.0\n",
      "Average Reward of all training: -20.333333333333332\n",
      "Max reward for a batch so far: -20.0\n",
      "===============================\n",
      "Epoch:  4 / 10\n",
      "Number of training episodes: 1\n",
      "Total reward: -21.0\n",
      "Mean Reward of that batch -21.0\n",
      "Average Reward of all training: -20.5\n",
      "Max reward for a batch so far: -20.0\n"
     ]
    }
   ],
   "source": [
    "#===================\n",
    "# Training and Printing some stats\n",
    "#===================\n",
    "allRewards =[]\n",
    "total_rewards =0\n",
    "maximumRewardRecorded =0\n",
    "mean_reward_total =[]\n",
    "num_epochs =10\n",
    "average_reward=[]\n",
    "training = True\n",
    "epoch = 1\n",
    "seed = 666\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    while epoch < num_epochs +1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(seed,1000)\n",
    "        # total rewards of the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # calculate the mean reward of the batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # calculate the average reward of all training\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # maximum reward recorded\n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"===============================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "        \n",
    "        # Jieda note: update the ValueNetwork parameter, and make an estimate of Value function for each given St\n",
    "        feed_dict_value = {ValueNetwork.inputs:states_mb, ValueNetwork.target:discounted_rewards_mb}\n",
    "        _, loss = sess.run([ValueNetwork.train_opt, ValueNetwork.loss], feed_dict_value)\n",
    "        \n",
    "        # Now make a prediction of value function using the updated ValueNetwork parameters    \n",
    "        value_prediction = sess.run(ValueNetwork.value_estimate, {ValueNetwork.inputs: states_mb})\n",
    "\n",
    "        # Jieda note: compute the discounted total rewards minus baseline\n",
    "        # shape of discounted_rewards_mb should be an array of length (number of 4 tuples in that batch)\n",
    "        # shape of value_prediction should also be an array of length (number of 4 tuples in that batch)\n",
    "        # Note: 4 tuples are essentially samples (st,at,rt,st+1)\n",
    "        discounted_rewards_mb_minus_baseline = discounted_rewards_mb - value_prediction\n",
    "\n",
    "        # feedforward, gradient and backprop\n",
    "        loss_,_ = sess.run([PolicyNetwork.loss,PolicyNetwork.train_opt], \\\n",
    "                feed_dict={PolicyNetwork.inputs: states_mb, PolicyNetwork.actions: actions_mb, PolicyNetwork.discounted_episode_rewards: discounted_rewards_mb_minus_baseline})\n",
    "\n",
    "        # update epoch\n",
    "        epoch +=1\n",
    "        \n",
    "        # Every 100 epoch, we write the mean_reward_total to a text file (over-write)\n",
    "        if (epoch % 2) ==0:\n",
    "            with open('mean_batch_reward_for_each_epoch.txt', 'w') as file:\n",
    "                file.write('%s\\n' % mean_reward_total)\n",
    "        \n",
    "        if (epoch % 5) == 0:\n",
    "            plt.plot(range(0,len(mean_reward_total)), mean_reward_total)\n",
    "            plt.savefig('./pong_basline_best_params_epoch'+str(epoch)+'.png')\n",
    "        \n",
    "\n",
    "# plot the average episode reward vs epoch\n",
    "# episode reward is the total undiscounted reward for an episode\n",
    "plt.plot(range(0,len(mean_reward_total)), mean_reward_total)\n",
    "plt.savefig('./pong_basline_best_params_final.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
